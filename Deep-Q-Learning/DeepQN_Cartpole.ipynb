{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for step in range(1000):\n",
    "    env.render(mode=\"human\")\n",
    "    random_action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(random_action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 observations\n",
    "num_observations = env.observation_space.shape[0]\n",
    "num_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input shape= 4 ANN ---> neurons = actions\n",
    "model= tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=num_actions, input_shape=(1,num_observations), activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_actions, activation='linear') # Neurons == actions_space\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_23 (Dense)            (None, 1, 1, 2)           10        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1, 1, 32)          96        \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1, 1, 2)           66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172\n",
      "Trainable params: 172\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = tf.keras.models.clone_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300 \n",
    "epsilon = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "LEARNING_RATE = 0.001\n",
    "DISCOUNT_FACTOR = 0.95\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(model, epsilon, observation):\n",
    "    if np.random.random() > epsilon:\n",
    "        prediction = model.predict(observation.reshape(1,1,4), verbose=0)  # perform the prediction on the observation\n",
    "        action = np.argmax(prediction)  # Chose the action with the higher value\n",
    "    else:\n",
    "        action = np.random.randint(0, env.action_space.n)  # Else use random action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = deque(maxlen=20000) \n",
    "update_target_model_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(replay_buffer, batch_size, model, target_model):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return\n",
    "    minibatch = random.sample(replay_buffer, batch_size)\n",
    "    target_batch = []\n",
    "\n",
    "    zipped_samples = list(zip(*minibatch))\n",
    "    states, actions, rewards, new_states, dones = zipped_samples\n",
    "\n",
    "    targets = target_model.predict(np.array(states), verbose= 0)\n",
    "\n",
    "    q_values = model.predict(np.array(new_states), verbose= 0)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        q_value = max(q_values[i][0])\n",
    "        target = targets[i].copy()\n",
    "        if dones[i]:\n",
    "            target[0][actions[i]]= rewards[i]\n",
    "        else:\n",
    "            target[0][actions[i]]= rewards[i] + DISCOUNT_FACTOR * q_value \n",
    "        target_batch.append(target)\n",
    "    \n",
    "    model.fit(np.array(states), np.array(target_batch), epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_handler(epoch, update_target_model_every, model, target_model):\n",
    "    if epoch > 0 and epoch % update_target_model_every == 0:\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mse', optimizer = tf.keras.optimizers.Adam(LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/300 | Points: 13 | Epsilon: 0.918316468354365 |BSF: 13\n",
      "Epoch: 25/300 | Points: 14 | Epsilon: 0.810157377815473 |BSF: 33\n",
      "Epoch: 50/300 | Points: 13 | Epsilon: 0.7147372386831305 |BSF: 46\n",
      "Epoch: 75/300 | Points: 15 | Epsilon: 0.6305556603555866 |BSF: 70\n",
      "Epoch: 100/300 | Points: 12 | Epsilon: 0.5562889678716474 |BSF: 70\n",
      "Epoch: 125/300 | Points: 21 | Epsilon: 0.4907693883854626 |BSF: 70\n",
      "Epoch: 150/300 | Points: 86 | Epsilon: 0.43296668905325736 |BSF: 177\n"
     ]
    }
   ],
   "source": [
    "best_so_far = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    observation = env.reset()\n",
    "\n",
    "    # (1,X) [a,b,c,d].reshape(1,4)\n",
    "    observation = observation.reshape([1,4])\n",
    "    done = False\n",
    "    points = 0\n",
    "\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(model, epsilon, observation)\n",
    "\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        new_observation = new_observation.reshape([1,4])\n",
    "\n",
    "        replay_buffer.append((observation, action, reward, new_observation, done))\n",
    "\n",
    "        observation = new_observation\n",
    "        points += 1\n",
    "\n",
    "        replay(replay_buffer, 32, model, target_model)\n",
    "        \n",
    "\n",
    "    epsilon = EPSILON_DECAY * epsilon\n",
    "    update_model_handler(epoch, update_target_model_every, model, target_model)\n",
    "\n",
    "    if points > best_so_far:\n",
    "        best_so_far = points\n",
    "\n",
    "    if epoch % 25  == 0:\n",
    "        print(\"Epoch: {}/{} | Points: {} | Epsilon: {} |BSF: {}\".format(epoch, EPOCHS, points, epsilon, best_so_far))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Reinforcement Learning\\Deep-Q-Learning\\DeepQN_Cartpole.ipynb Zelle 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Reinforcement%20Learning/Deep-Q-Learning/DeepQN_Cartpole.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m300\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Reinforcement%20Learning/Deep-Q-Learning/DeepQN_Cartpole.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     env\u001b[39m.\u001b[39mrender(mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Reinforcement%20Learning/Deep-Q-Learning/DeepQN_Cartpole.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(model\u001b[39m.\u001b[39mpredict(observation\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m), verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Reinforcement%20Learning/Deep-Q-Learning/DeepQN_Cartpole.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Reinforcement%20Learning/Deep-Q-Learning/DeepQN_Cartpole.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m done:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "\n",
    "for step in range(300):\n",
    "    env.render(mode=\"human\")\n",
    "    action = np.argmax(model.predict(observation.reshape(1,1,4), verbose=0))\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
