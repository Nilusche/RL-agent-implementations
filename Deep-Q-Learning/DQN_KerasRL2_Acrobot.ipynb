{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Acrobot-v1'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for step in range(200):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space.n\n",
    "print(n_actions)\n",
    "n_obs = env.observation_space.shape\n",
    "print(n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 6)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                448       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,963\n",
      "Trainable params: 8,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1,) + n_obs),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),   \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(n_actions, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=150000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=n_actions, memory=memory, nb_steps_warmup=1000, batch_size=32, target_model_update=1000, policy=policy, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dqn.compile(tf.keras.optimizers.Adam(lr=0.001), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "   79/10000 [..............................] - ETA: 12s - reward: -1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 71s 7ms/step - reward: -0.9997\n",
      "20 episodes - episode_reward: -486.450 [-500.000, -310.000] - loss: 0.024 - mae: 3.335 - mean_q: -4.918 - mean_eps: 0.967\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -1.0000\n",
      "20 episodes - episode_reward: -500.000 [-500.000, -500.000] - loss: 0.144 - mae: 8.338 - mean_q: -12.316 - mean_eps: 0.910\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 76s 8ms/step - reward: -0.9997\n",
      "21 episodes - episode_reward: -488.143 [-500.000, -369.000] - loss: 0.327 - mae: 12.602 - mean_q: -18.643 - mean_eps: 0.850\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -0.9986\n",
      "23 episodes - episode_reward: -423.261 [-500.000, -237.000] - loss: 0.485 - mae: 16.112 - mean_q: -23.846 - mean_eps: 0.790\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -0.9976\n",
      "27 episodes - episode_reward: -365.259 [-500.000, -204.000] - loss: 0.677 - mae: 19.024 - mean_q: -28.106 - mean_eps: 0.730\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -0.9968\n",
      "33 episodes - episode_reward: -312.061 [-500.000, -180.000] - loss: 0.777 - mae: 21.458 - mean_q: -31.618 - mean_eps: 0.670\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -0.9959\n",
      "41 episodes - episode_reward: -242.000 [-471.000, -164.000] - loss: 0.758 - mae: 23.433 - mean_q: -34.482 - mean_eps: 0.610\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 80s 8ms/step - reward: -0.9949\n",
      "51 episodes - episode_reward: -196.255 [-384.000, -134.000] - loss: 0.780 - mae: 24.610 - mean_q: -36.077 - mean_eps: 0.550\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -0.9936\n",
      "64 episodes - episode_reward: -154.875 [-264.000, -105.000] - loss: 0.657 - mae: 24.261 - mean_q: -35.462 - mean_eps: 0.490\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 79s 8ms/step - reward: -0.9930\n",
      "70 episodes - episode_reward: -141.029 [-209.000, -98.000] - loss: 0.594 - mae: 24.317 - mean_q: -35.469 - mean_eps: 0.430\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -0.9923\n",
      "77 episodes - episode_reward: -129.000 [-226.000, -89.000] - loss: 0.576 - mae: 23.902 - mean_q: -34.807 - mean_eps: 0.370\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 75s 8ms/step - reward: -0.9917\n",
      "83 episodes - episode_reward: -118.711 [-184.000, -81.000] - loss: 0.586 - mae: 23.091 - mean_q: -33.544 - mean_eps: 0.310\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 78s 8ms/step - reward: -0.9911\n",
      "89 episodes - episode_reward: -112.944 [-263.000, -70.000] - loss: 0.650 - mae: 22.142 - mean_q: -32.077 - mean_eps: 0.250\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      "10000/10000 [==============================] - 77s 8ms/step - reward: -0.9899\n",
      "101 episodes - episode_reward: -96.911 [-149.000, -62.000] - loss: 0.635 - mae: 21.413 - mean_q: -30.936 - mean_eps: 0.190\n",
      "\n",
      "Interval 15 (140000 steps performed)\n",
      "10000/10000 [==============================] - 73s 7ms/step - reward: -0.9892\n",
      "done, took 1147.872 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x247071df4f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_{}_weights.h5f'.format(env_name), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -71.000, steps: 72\n",
      "Episode 2: reward: -84.000, steps: 85\n",
      "Episode 3: reward: -110.000, steps: 111\n",
      "Episode 4: reward: -81.000, steps: 82\n",
      "Episode 5: reward: -81.000, steps: 82\n"
     ]
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
