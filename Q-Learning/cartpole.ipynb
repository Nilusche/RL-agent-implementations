{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet in c:\\users\\nilus\\anaconda3\\lib\\site-packages (1.5.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install ale-py==0.7\n",
    "!pip install gym==0.19 \n",
    "!pip install gym[all]\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from gym.envs.registration import register\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02793337, -0.01649851,  0.02513945, -0.04193092])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for step in range(100):\n",
    "    env.render();\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, infor = env.step(action)\n",
    "    time.sleep(0.02)\n",
    "\n",
    "env.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(num_bins_per_action = 10):\n",
    "    bins_cart_position = np.linspace(-4.8, 4.8, num_bins_per_action)\n",
    "    bins_cart_velocity = np.linspace(-5, 5, num_bins_per_action)\n",
    "    bins_pole_angle = np.linspace(-0.418, 0.418, num_bins_per_action)\n",
    "    bins_pole_velocity = np.linspace(-5, 5, num_bins_per_action)\n",
    "    bins = np.array([bins_cart_position, bins_cart_velocity, bins_pole_angle, bins_pole_velocity])\n",
    "    return bins\n",
    "\n",
    "NUM_BINS = 10\n",
    "BINS = create_bins(NUM_BINS)\n",
    "\n",
    "def discretize_observation(observations, bins):\n",
    "    binned_observations = []\n",
    "    for i , observation in enumerate(observations):\n",
    "        binned_observations.append(np.digitize(observation, bins[i]))\n",
    "    \n",
    "    return tuple(binned_observations)\n",
    "\n",
    "#observations = env.reset()\n",
    "#binned_observations = discretize_observation(observations, BINS)\n",
    "#print(binned_observations)\n",
    "\n",
    "q_table_shape  = (NUM_BINS, NUM_BINS, NUM_BINS, NUM_BINS, env.action_space.n)\n",
    "q_table = np.zeros(q_table_shape)\n",
    "\n",
    "\n",
    "\n",
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    # EXPLORATION\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return env.action_space.sample()\n",
    "    # EXPLOITATION\n",
    "    else:\n",
    "        return np.argmax(q_table[discrete_state])\n",
    "\n",
    "EPOCHS = 14000 # number of episodes\n",
    "ALPHA = 0.8 # Learning rate\n",
    "GAMMA = 0.9 # Discount factor\n",
    "\n",
    "def compute_next_q_value(old_q_value, reward, new_q_value):\n",
    "    return old_q_value + ALPHA * (reward + GAMMA * new_q_value - old_q_value)\n",
    "\n",
    "epsilon = 1\n",
    "BURN_IN = 1\n",
    "EPSILON_END =10000\n",
    "EPSILON_REDUCE = 0.0001\n",
    "\n",
    "def reduce_epsilon(epsilon, epoch):\n",
    "    if BURN_IN <= epoch < EPSILON_END:\n",
    "        return epsilon - EPSILON_REDUCE\n",
    "    return epsilon\n",
    "\n",
    "def fail(done, points, reward):\n",
    "    if done and points < 150:\n",
    "        reward = -200\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x00000190375634D0>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "epsilon =1.0\n",
    "rewards = []\n",
    "log_interval = 500\n",
    "render_interval = 10000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "fig.canvas.draw()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "points_log = []\n",
    "mean_points_log = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    initial_state = env.reset()\n",
    "    discrete_state = discretize_observation(initial_state, BINS)\n",
    "    done = False\n",
    "    points = 0\n",
    "\n",
    "    epochs.append(epoch)\n",
    "    while not done:\n",
    "        action = epsilon_greedy_action_selection(epsilon, q_table, discrete_state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        reward = fail(done, points, reward)\n",
    "\n",
    "        next_state_discrete = discretize_observation(next_state, BINS)\n",
    "\n",
    "        old_q_value = q_table[discrete_state + (action,)]\n",
    "        next_optimal_q_value = np.max(q_table[next_state_discrete])\n",
    "\n",
    "        next_q_value = compute_next_q_value(old_q_value, reward, next_optimal_q_value)\n",
    "        q_table[discrete_state + (action,)] = next_q_value\n",
    "\n",
    "        discrete_state = next_state_discrete\n",
    "        points += 1\n",
    "\n",
    "    epsilon = reduce_epsilon(epsilon, epoch)\n",
    "    points_log.append(points)\n",
    "    running_mean = round(np.mean(points_log[-30:]), 2)\n",
    "    mean_points_log.append(running_mean)\n",
    "\n",
    "    if epoch % log_interval == 0:\n",
    "        \n",
    "        ax.clear()\n",
    "        ax.scatter(epochs, points_log)\n",
    "        ax.plot(epochs, points_log)\n",
    "        ax.plot(epochs, mean_points_log, label='Running Mean')\n",
    "        plt.legend()\n",
    "        fig.canvas.draw()\n",
    "        plt.pause(0.01) \n",
    "        \n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "rewards = 0\n",
    "for step in range(1000):\n",
    "    env.render()\n",
    "    discrete_state = discretize_observation(observation, BINS)\n",
    "    action = np.argmax(q_table[discrete_state])\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards += 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d74c2a98a895d9dbf296125ba4db6f84070f8a438950a89a054a418443e79409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
